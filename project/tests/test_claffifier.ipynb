{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/Walk_Video_PyTorch/project\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "import os \n",
    "# os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "\n",
    "%cd ../"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.9/dist-packages/numpy/core/getlimits.py:499: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
      "  setattr(self, word, getattr(machar, word).flat[0])\n",
      "/usr/local/lib/python3.9/dist-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float32'> type is zero.\n",
      "  return self._float_to_str(self.smallest_subnormal)\n",
      "/usr/local/lib/python3.9/dist-packages/numpy/core/getlimits.py:499: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
      "  setattr(self, word, getattr(machar, word).flat[0])\n",
      "/usr/local/lib/python3.9/dist-packages/numpy/core/getlimits.py:89: UserWarning: The value of the smallest subnormal for <class 'numpy.float64'> type is zero.\n",
      "  return self._float_to_str(self.smallest_subnormal)\n"
     ]
    }
   ],
   "source": [
    "from  models.pytorchvideo_models import WalkVideoClassificationLightningModule\n",
    "from pytorch_lightning import Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from main import get_parameters\n",
    "\n",
    "opt, unknown = get_parameters()\n",
    "opt.num_workers = 2\n",
    "opt.batch_size = 16\n",
    "opt.gpu_num = 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = WalkVideoClassificationLightningModule(opt)\n",
    "# model = model.load_from_checkpoint(\"/workspace/Walk_Video_PyTorch/logs/resnet/0603/checkpoints/epoch=99-step=1800.ckpt\")\n",
    "model.eval()\n",
    "\n",
    "checkpoint = torch.load(\"/workspace/Walk_Video_PyTorch/logs/resnet/0719_resnet_depth50/checkpoints/epoch=41-step=4704.ckpt\")\n",
    "\n",
    "model.load_state_dict(checkpoint[\"state_dict\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "/workspace/data/dataset/predict not found.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/workspace/Walk_Video_PyTorch/project/tests/test_claffifier.ipynb Cell 5\u001b[0m in \u001b[0;36m<cell line: 6>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f766964656f5f74646a696e222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6b61746179616d615f686f6d65227d7d/workspace/Walk_Video_PyTorch/project/tests/test_claffifier.ipynb#ch0000004vscode-remote?line=3'>4</a>\u001b[0m \u001b[39m# load test dataset \u001b[39;00m\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f766964656f5f74646a696e222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6b61746179616d615f686f6d65227d7d/workspace/Walk_Video_PyTorch/project/tests/test_claffifier.ipynb#ch0000004vscode-remote?line=4'>5</a>\u001b[0m module \u001b[39m=\u001b[39m WalkDataModule(opt)\n\u001b[0;32m----> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f766964656f5f74646a696e222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6b61746179616d615f686f6d65227d7d/workspace/Walk_Video_PyTorch/project/tests/test_claffifier.ipynb#ch0000004vscode-remote?line=5'>6</a>\u001b[0m module\u001b[39m.\u001b[39;49msetup()\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f766964656f5f74646a696e222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6b61746179616d615f686f6d65227d7d/workspace/Walk_Video_PyTorch/project/tests/test_claffifier.ipynb#ch0000004vscode-remote?line=6'>7</a>\u001b[0m test_data \u001b[39m=\u001b[39m module\u001b[39m.\u001b[39mtest_dataloader()\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f766964656f5f74646a696e222c2273657474696e6773223a7b22686f7374223a227373683a2f2f6b61746179616d615f686f6d65227d7d/workspace/Walk_Video_PyTorch/project/tests/test_claffifier.ipynb#ch0000004vscode-remote?line=8'>9</a>\u001b[0m \u001b[39m# for the tensorboard\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/Walk_Video_PyTorch/project/dataloader/data_loader.py:143\u001b[0m, in \u001b[0;36mWalkDataModule.setup\u001b[0;34m(self, stage)\u001b[0m\n\u001b[1;32m    141\u001b[0m \u001b[39m# FIXME\u001b[39;00m\n\u001b[1;32m    142\u001b[0m \u001b[39mif\u001b[39;00m stage \u001b[39min\u001b[39;00m (\u001b[39m\"\u001b[39m\u001b[39mpredict\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mNone\u001b[39;00m):\n\u001b[0;32m--> 143\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtest_pred_dataset \u001b[39m=\u001b[39m WalkDataset(\n\u001b[1;32m    144\u001b[0m         data_path\u001b[39m=\u001b[39;49mos\u001b[39m.\u001b[39;49mpath\u001b[39m.\u001b[39;49mjoin(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_SPLIT_DATA_PATH, \u001b[39m\"\u001b[39;49m\u001b[39mpredict\u001b[39;49m\u001b[39m\"\u001b[39;49m),\n\u001b[1;32m    145\u001b[0m         clip_sampler\u001b[39m=\u001b[39;49mpytorchvideo\u001b[39m.\u001b[39;49mdata\u001b[39m.\u001b[39;49mmake_clip_sampler(\u001b[39m\"\u001b[39;49m\u001b[39mrandom\u001b[39;49m\u001b[39m\"\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_CLIP_DURATION),\n\u001b[1;32m    146\u001b[0m         transform\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtransform\n\u001b[1;32m    147\u001b[0m     )\n",
      "File \u001b[0;32m/workspace/Walk_Video_PyTorch/project/dataloader/data_loader.py:61\u001b[0m, in \u001b[0;36mWalkDataset\u001b[0;34m(data_path, clip_sampler, video_sampler, transform, video_path_prefix, decode_audio, decoder)\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mWalkDataset\u001b[39m(\n\u001b[1;32m     37\u001b[0m     data_path: \u001b[39mstr\u001b[39m,\n\u001b[1;32m     38\u001b[0m     clip_sampler: ClipSampler,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     43\u001b[0m     decoder: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpyav\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     44\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m LabeledVideoDataset:\n\u001b[1;32m     45\u001b[0m     \u001b[39m'''\u001b[39;00m\n\u001b[1;32m     46\u001b[0m \u001b[39m    A helper function to create \"LabeledVideoDataset\" object for the Walk dataset.\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[39m        LabeledVideoDataset: _description_\u001b[39;00m\n\u001b[1;32m     60\u001b[0m \u001b[39m    '''\u001b[39;00m\n\u001b[0;32m---> 61\u001b[0m     \u001b[39mreturn\u001b[39;00m labeled_video_dataset(\n\u001b[1;32m     62\u001b[0m         data_path,\n\u001b[1;32m     63\u001b[0m         clip_sampler,\n\u001b[1;32m     64\u001b[0m         video_sampler,\n\u001b[1;32m     65\u001b[0m         transform,\n\u001b[1;32m     66\u001b[0m         video_path_prefix,\n\u001b[1;32m     67\u001b[0m         decode_audio,\n\u001b[1;32m     68\u001b[0m         decoder\n\u001b[1;32m     69\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pytorchvideo/data/labeled_video_dataset.py:286\u001b[0m, in \u001b[0;36mlabeled_video_dataset\u001b[0;34m(data_path, clip_sampler, video_sampler, transform, video_path_prefix, decode_audio, decoder)\u001b[0m\n\u001b[1;32m    244\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mlabeled_video_dataset\u001b[39m(\n\u001b[1;32m    245\u001b[0m     data_path: \u001b[39mstr\u001b[39m,\n\u001b[1;32m    246\u001b[0m     clip_sampler: ClipSampler,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    251\u001b[0m     decoder: \u001b[39mstr\u001b[39m \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mpyav\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    252\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m LabeledVideoDataset:\n\u001b[1;32m    253\u001b[0m     \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    254\u001b[0m \u001b[39m    A helper function to create ``LabeledVideoDataset`` object for Ucf101 and Kinetics datasets.\u001b[39;00m\n\u001b[1;32m    255\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    284\u001b[0m \n\u001b[1;32m    285\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 286\u001b[0m     labeled_video_paths \u001b[39m=\u001b[39m LabeledVideoPaths\u001b[39m.\u001b[39;49mfrom_path(data_path)\n\u001b[1;32m    287\u001b[0m     labeled_video_paths\u001b[39m.\u001b[39mpath_prefix \u001b[39m=\u001b[39m video_path_prefix\n\u001b[1;32m    288\u001b[0m     dataset \u001b[39m=\u001b[39m LabeledVideoDataset(\n\u001b[1;32m    289\u001b[0m         labeled_video_paths,\n\u001b[1;32m    290\u001b[0m         clip_sampler,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    294\u001b[0m         decoder\u001b[39m=\u001b[39mdecoder,\n\u001b[1;32m    295\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.9/dist-packages/pytorchvideo/data/labeled_video_paths.py:34\u001b[0m, in \u001b[0;36mLabeledVideoPaths.from_path\u001b[0;34m(cls, data_path)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[39mreturn\u001b[39;00m LabeledVideoPaths\u001b[39m.\u001b[39mfrom_directory(data_path)\n\u001b[1;32m     33\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m---> 34\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mFileNotFoundError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00mdata_path\u001b[39m}\u001b[39;00m\u001b[39m not found.\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: /workspace/data/dataset/predict not found."
     ]
    }
   ],
   "source": [
    "from dataloader.data_loader import WalkDataModule\n",
    "from pytorch_lightning import loggers as pl_loggers \n",
    "\n",
    "# load test dataset \n",
    "module = WalkDataModule(opt)\n",
    "# module.setup()\n",
    "test_data = module.test_dataloader()\n",
    "\n",
    "# for the tensorboard\n",
    "tb_logger = pl_loggers.TensorBoardLogger(save_dir=\"/workspace/Walk_Video_PyTorch/project/tests/logs\", name=opt.model, version=opt.version)\n",
    "\n",
    "\n",
    "trainer = Trainer(\n",
    "    accelerator=\"auto\",\n",
    "    devices=1,\n",
    "    gpus=opt.gpu_num,\n",
    "    logger=tb_logger,\n",
    ")\n",
    "\n",
    "results = trainer.test(dataloaders=test_data, model=model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_data = next(iter(test_data))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['20180723_1_ASD_lat__V1-0074.mp4',\n",
       " '20180723_1_ASD_lat__V1-0043.mp4',\n",
       " '20180521_2_ASD_lat__V1-0073.mp4',\n",
       " '20180109_LCS_lat_V1-0052.mp4',\n",
       " '20160927_DHS_lat_V1-0004.mp4',\n",
       " '20171127_DHS_lat_V1-0041.mp4',\n",
       " '20170412_DHS_lat_V1-0004.mp4',\n",
       " '20190225_DHS_lat_V1-0046.mp4',\n",
       " '20190507_1_ASD_lat__V1-0018.mp4',\n",
       " '20210427_ASD_lat_V1-0002.mp4',\n",
       " '20180723_1_ASD_lat__V1-0014.mp4',\n",
       " '20170130_ASD_lat_ (12).mp4',\n",
       " '20190507_2_ASD_lat__V1-0015.mp4',\n",
       " '20210406_ASD_lat_V1-0025.mp4',\n",
       " '20191029_1_ASD_lat__V1-0005.mp4',\n",
       " '20191217_DHS_lat_V1-0013.mp4']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data['video'].shape\n",
    "\n",
    "input_data[\"video_name\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 1])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_data['label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = model(input_data['video'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_act = torch.nn.Softmax(dim=1)\n",
    "preds = post_act(preds)\n",
    "pred_classes = preds.topk(k=1).indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [1],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [0],\n",
       "        [1]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "classname = {}\n",
    "\n",
    "classname[0] = 'asd'\n",
    "classname[1] = 'asd_not'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "real_calss = []\n",
    "\n",
    "for i in input_data['label'].tolist():\n",
    "    real_calss.append(classname[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['asd',\n",
       " 'asd',\n",
       " 'asd',\n",
       " 'asd_not',\n",
       " 'asd_not',\n",
       " 'asd_not',\n",
       " 'asd_not',\n",
       " 'asd_not',\n",
       " 'asd',\n",
       " 'asd',\n",
       " 'asd',\n",
       " 'asd',\n",
       " 'asd',\n",
       " 'asd',\n",
       " 'asd',\n",
       " 'asd_not']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_calss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted labels: asd, asd, asd, asd_not, asd_not, asd_not, asd_not, asd_not, asd, asd, asd, asd, asd, asd, asd, asd_not\n",
      "real label: asd,asd,asd,asd_not,asd_not,asd_not,asd_not,asd_not,asd,asd,asd,asd,asd,asd,asd,asd_not\n"
     ]
    }
   ],
   "source": [
    "# pred_class_names = []\n",
    "# for num in range(opt._BATCH_SIZE):\n",
    "#     for i in pred_classes[i]:\n",
    "#         pred_class_names.append(classname[int(i)])\n",
    "\n",
    "\n",
    "pred_class_names = [classname[int(i)] for i in pred_classes]\n",
    "print(\"Predicted labels: %s\" % \", \".join(pred_class_names))\n",
    "print(\"real label: %s\" % \",\".join(real_calss))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred_class_names == real_calss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['true',\n",
       " 'true',\n",
       " 'true',\n",
       " 'true',\n",
       " 'true',\n",
       " 'true',\n",
       " 'true',\n",
       " 'true',\n",
       " 'true',\n",
       " 'true',\n",
       " 'true',\n",
       " 'true',\n",
       " 'true',\n",
       " 'true',\n",
       " 'true',\n",
       " 'true']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "result = []\n",
    "\n",
    "for i in range(len(real_calss)):\n",
    "    if pred_class_names[i] == real_calss[i]:\n",
    "        result.append(\"true\")\n",
    "    else:\n",
    "        result.append(\"fale\")\n",
    "\n",
    "result"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
