{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd /workspace/Walk_Video_PyTorch/project\n",
    "\n",
    "from main import get_parameters\n",
    "from IPython.display import clear_output\n",
    "\n",
    "config, unkonwn = get_parameters()\n",
    "\n",
    "config.clip_duration = 2\n",
    "config.num_workers = 3\n",
    "config.batch_size = 4\n",
    "config.gpu_num = 1\n",
    "config.img_size = 256\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any, Callable, Dict, Optional, Type\n",
    "\n",
    "import os \n",
    "import torch \n",
    "import pytorchvideo\n",
    "from pytorchvideo.data.clip_sampling import ClipSampler\n",
    "\n",
    "from pytorchvideo.data.labeled_video_dataset import LabeledVideoDataset\n",
    "from pytorchvideo.data import labeled_video_dataset, make_clip_sampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def WalkDataset(\n",
    "    data_path: str,\n",
    "    clip_sampler: ClipSampler,\n",
    "    video_sampler: Type[torch.utils.data.Sampler] = torch.utils.data.RandomSampler,\n",
    "    transform: Optional[Callable[[Dict[str, Any]], Dict[str, Any]]] = None,\n",
    "    video_path_prefix: str = \"\",\n",
    "    decode_audio: bool = False,\n",
    "    decoder: str = \"pyav\",\n",
    ") -> LabeledVideoDataset:\n",
    "    '''\n",
    "    A helper function to create \"LabeledVideoDataset\" object for the Walk dataset.\n",
    "\n",
    "    Args:\n",
    "        data_path (str): Path to the data. The path defines how the data should be read. For a directory, the directory structure defines the classes (i.e. each subdirectory is class).\n",
    "        clip_sampler (ClipSampler): Defines how clips should be sampled from each video. See the clip sampling documentation for more information.\n",
    "        video_sampler (Type[torch.utils.data.Sampler], optional): Sampler for the internal video container. Defaults to torch.utils.data.RandomSampler.\n",
    "        transform (Optional[Callable[[Dict[str, Any]], Dict[str, Any]]], optional): This callable is evaluated on the clip output before the clip is returned. Defaults to None.\n",
    "        video_path_prefix (str, optional): Path to root directory with the videos that are\n",
    "                loaded in ``LabeledVideoDataset``. Defaults to \"\".\n",
    "        decode_audio (bool, optional): If True, also decode audio from video. Defaults to False. Notice that, if Ture will trigger the stack error.\n",
    "        decoder (str, optional): Defines what type of decoder used to decode a video. Defaults to \"pyav\".\n",
    "\n",
    "    Returns:\n",
    "        LabeledVideoDataset: _description_\n",
    "    '''\n",
    "    return labeled_video_dataset(\n",
    "        data_path,\n",
    "        clip_sampler,\n",
    "        video_sampler,\n",
    "        transform,\n",
    "        video_path_prefix,\n",
    "        decode_audio,\n",
    "        decoder\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorchvideo.transforms import (\n",
    "    ApplyTransformToKey,\n",
    "    Normalize,\n",
    "    RemoveKey,\n",
    "    ShortSideScale,\n",
    "    RandomShortSideScale,\n",
    "    UniformTemporalSubsample,\n",
    "    Div255,\n",
    "    RandomResizedCrop,\n",
    "    create_video_transform\n",
    ")\n",
    "\n",
    "from torchvision.transforms import (\n",
    "    Compose,\n",
    "    Lambda,\n",
    "    RandomCrop,\n",
    "    RandomHorizontalFlip,\n",
    "    Resize,\n",
    "    ToTensor,\n",
    "    \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyexpat import model\n",
    "from venv import create\n",
    "\n",
    "\n",
    "class WalkDataModule():\n",
    "    def __init__(self, opt):\n",
    "        super().__init__()\n",
    "        self._DATA_PATH = opt.split_data_path\n",
    "        self._CLIP_DURATION = opt.clip_duration\n",
    "        self._BATCH_SIZE = opt.batch_size\n",
    "        self._NUM_WORKERS = opt.num_workers\n",
    "    \n",
    "    def train_dataloader(self):\n",
    "        '''\n",
    "        create the Walk train partition from the list of video labels \n",
    "        in directory and subdirectory. Add transform that subsamples and \n",
    "        normalizes the video before applying the scale, crop and flip augmentations.\n",
    "        '''        \n",
    "        train_transform = Compose(\n",
    "            [\n",
    "                ApplyTransformToKey(\n",
    "                    key=\"video\",\n",
    "                    transform=Compose(\n",
    "                        [\n",
    "                            # \n",
    "                            UniformTemporalSubsample(30),\n",
    "                            Div255(),\n",
    "                            Normalize((0.45, 0.45, 0.45), (0.225, 0.225, 0.225)),\n",
    "                            \n",
    "                            # RandomShortSideScale(min_size=256, max_size=320),\n",
    "                            # ShortSideScale(256),\n",
    "                            # RandomCrop(244),\n",
    "                            Resize(size=[256, 256]),\n",
    "                            # RandomResizedCrop(512, 512, scale=0.5, aspect_ratio=0.5),\n",
    "                            RandomHorizontalFlip(p=0.5),\n",
    "                        ]\n",
    "                    ),\n",
    "                ),\n",
    "                # create_video_transform(mode='train', video_key='video', num_samples=8, crop_size=(256,256), convert_to_float=False)\n",
    "            ]\n",
    "        )\n",
    "\n",
    "        # train_transform = create_video_transform(mode='train', num_samples=10, crop_size=(256, 256))\n",
    "\n",
    "        train_dataset = WalkDataset(\n",
    "            data_path= os.path.join(self._DATA_PATH, 'train'),\n",
    "            clip_sampler=make_clip_sampler(\"uniform\", self._CLIP_DURATION),\n",
    "            transform=train_transform\n",
    "        )\n",
    "\n",
    "        return torch.utils.data.DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size = self._BATCH_SIZE,\n",
    "            num_workers = self._NUM_WORKERS,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_module = WalkDataModule(config).train_dataloader()\n",
    "\n",
    "batch = next(iter(data_module))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch['video_name'][0]\n",
    "\n",
    "batch_value = {}\n",
    "\n",
    "batch.keys()\n",
    "\n",
    "for keys in batch.keys():\n",
    "    if keys == 'video':\n",
    "        batch_value[keys] = batch[keys][0].size()\n",
    "    else:\n",
    "        batch_value[keys] = batch[keys][0]\n",
    "\n",
    "batch_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch['video_name']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch[\"video\"][0].permute(1, 2, 3, 0)[0].shape\n",
    "\n",
    "batch['video'][0][0][0][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pylab as plt \n",
    "\n",
    "print(batch['video_name'][0])\n",
    "\n",
    "plt.figure(figsize=(150, 150))\n",
    "\n",
    "for i in range(batch['video'][0].size()[1]):\n",
    "    plt.title(batch['video_name'][0])\n",
    "    plt.subplot(1, batch['video'][0].size()[1], i + 1)\n",
    "    plt.imshow(batch['video'][0].permute(1, 2, 3, 0)[i])\n",
    "\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import matplotlib.pylab as plt \n",
    "\n",
    "# print(batch['video_name'][0])\n",
    "\n",
    "# location = 1\n",
    "\n",
    "# plt.figure(figsize=(244, 244))\n",
    "\n",
    "# for num in range(batch['video'].size()[0]): # batch size\n",
    "#     for i in range(batch['video'].size()[2]): # 帧数\n",
    "#         plt.title(batch['video_name'][num])\n",
    "            \n",
    "#         plt.imshow(batch[\"video\"][num].permute(1, 2, 3, 0)[i])\n",
    "\n",
    "#         location +=1 \n",
    "#         plt.axis(\"off\")\n",
    "\n",
    "# plt.show()\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "767d51c1340bd893661ea55ea3124f6de3c7a262a8b4abca0554b478b1e2ff90"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
