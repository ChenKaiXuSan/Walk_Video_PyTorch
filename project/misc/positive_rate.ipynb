{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspace/Walk_Video_PyTorch/project\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from pytorchvideo.transforms import (\n",
    "    ApplyTransformToKey,\n",
    "    Normalize,\n",
    "    RandomShortSideScale,\n",
    "    ShortSideScale,\n",
    "    UniformTemporalSubsample,\n",
    "    Div255,\n",
    ")\n",
    "from torchvision.transforms import (\n",
    "    Compose,\n",
    "    Lambda,\n",
    "    RandomCrop,\n",
    "    Resize,\n",
    "    RandomHorizontalFlip,\n",
    ")\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"1\"\n",
    "\n",
    "\n",
    "%cd /workspace/Walk_Video_PyTorch/project\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Global seed set to 42\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "42"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from pytorch_lightning import seed_everything\n",
    "from torchmetrics.classification import BinaryAccuracy\n",
    "from models.pytorchvideo_models import WalkVideoClassificationLightningModule\n",
    "from dataloader.data_loader import WalkDataModule\n",
    "\n",
    "from pytorch_lightning import Trainer\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "clear_output()\n",
    "\n",
    "\n",
    "seed_everything(42, workers=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from parameters import get_parameters\n",
    "\n",
    "VIDEO_LENGTH = ['1', '2', '3']\n",
    "VIDEO_FRAME = ['8', '16', '32']\n",
    "\n",
    "opt, _ = get_parameters()\n",
    "opt.num_workers = 8\n",
    "opt.batch_size = 8\n",
    "opt.gpu_num = 1\n",
    "\n",
    "opt.version = '516_1_8'\n",
    "opt.model = \"resnet\"\n",
    "opt.model_depth = 50\n",
    "opt.model_class_num = 1\n",
    "\n",
    "# opt.clip_duration = 2\n",
    "# opt.uniform_temporal_subsample_num = 30\n",
    "opt.version = opt.version + '_' + opt.model + '_depth' + str(opt.model_depth)\n",
    "\n",
    "opt.fusion_method = 'slow_fusion'\n",
    "opt.fix_layer = 'all'\n",
    "# opt.train_path = '/workspace/data/split_pad_dataset_512/fold3/'\n",
    "\n",
    "opt.transfor_learning = True\n",
    "opt.pre_process_flag = True\n",
    "\n",
    "DATA_PATH = opt.split_pad_data_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "the data path: /workspace/data/split_pad_dataset_512/flod3\n",
      "ckpt: /workspace/Walk_Video_PyTorch/logs/resnet/516_1_8_resnet_depth50/flod0/checkpoints/epoch=17-val_loss=1.82-val_acc=0.7080.ckpt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /root/.cache/torch/hub/facebookresearch_pytorchvideo_main\n",
      "Using cache found in /root/.cache/torch/hub/facebookresearch_pytorchvideo_main\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##################################################\n",
      "run pre process model! /workspace/data/split_pad_dataset_512/flod3\n",
      "##################################################\n"
     ]
    }
   ],
   "source": [
    "opt.train_path = os.path.join(DATA_PATH, 'flod3')\n",
    "\n",
    "opt.clip_duration = int(1)\n",
    "opt.uniform_temporal_subsample_num = int(8)\n",
    "\n",
    "# ckpt_path = '/workspace/Walk_Video_PyTorch/logs/resnet/1119_1_8_resnet_depth50/flod3/checkpoints/epoch=23-val_loss=0.47-val_acc=0.8568.ckpt'\n",
    "ckpt_path = '/workspace/Walk_Video_PyTorch/logs/resnet/516_1_8_resnet_depth50/flod0/checkpoints/epoch=17-val_loss=1.82-val_acc=0.7080.ckpt'\n",
    "\n",
    "print('#' * 50)\n",
    "print('the data path: %s' % opt.train_path)\n",
    "print('ckpt: %s' % ckpt_path)\n",
    "model = WalkVideoClassificationLightningModule(opt).load_from_checkpoint(ckpt_path)\n",
    "\n",
    "data_module = WalkDataModule(opt)\n",
    "data_module.setup()\n",
    "test_data = data_module.val_dataloader()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = Compose(\n",
    "    [\n",
    "        ApplyTransformToKey(\n",
    "            key=\"video\",\n",
    "            transform=Compose(\n",
    "                [\n",
    "                    # uniform clip T frames from the given n sec video.\n",
    "                    UniformTemporalSubsample(8),\n",
    "\n",
    "                    # dived the pixel from [0, 255] tp [0, 1], to save computing resources.\n",
    "                    Div255(),\n",
    "                    Normalize((0.45, 0.45, 0.45), (0.225, 0.225, 0.225)),\n",
    "\n",
    "                    Resize(size=[224, 224]),\n",
    "                    # RandomHorizontalFlip(p=0.5),\n",
    "                ]\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_loader import WalkDataset\n",
    "from pytorchvideo.data import make_clip_sampler\n",
    "\n",
    "dset = WalkDataset(\n",
    "    data_path='/workspace/Walk_Video_PyTorch/project/misc/test_video/',\n",
    "    clip_sampler=make_clip_sampler('uniform', 1),\n",
    "    video_sampler=torch.utils.data.SequentialSampler,\n",
    "    transform=transform,\n",
    ")\n",
    "\n",
    "test_data = DataLoader(\n",
    "    dset,\n",
    "    batch_size=300\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(test_data))\n",
    "\n",
    "video = batch['video'].detach()  # b, c, t, h, w\n",
    "label = batch['label'].detach()  # b, class num\n",
    "label = [0] * len(label)\n",
    "name = batch['video_name']\n",
    "\n",
    "# video.shape, label, name, len(name)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pred the result \n",
    "use pre trained model to predict the reulst"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred the result\n",
    "model = model.cuda()\n",
    "video = video.cuda()\n",
    "\n",
    "with torch.no_grad():\n",
    "    pred = model(video)\n",
    "\n",
    "pred = pred.cpu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_list = []\n",
    "\n",
    "for i in torch.sigmoid(pred):\n",
    "    if i > 0.5:\n",
    "        pred_list.append(1)\n",
    "    else:\n",
    "        pred_list.append(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "60"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pred_list, label,\n",
    "len(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.7167)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# calc acc\n",
    "acc = BinaryAccuracy(threshold=0.5)\n",
    "\n",
    "acc(torch.sigmoid(pred).squeeze(), torch.tensor(label))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(0.7500)\n",
      "tensor(0.7500)\n",
      "tensor(0.6500)\n"
     ]
    }
   ],
   "source": [
    "acc(torch.sigmoid(pred[:65]).squeeze(), torch.tensor(label[:65]))\n",
    "for i in range(0, len(label), len(label)//3):\n",
    "    print(acc(torch.sigmoid(pred[i:i+len(label)//3]).squeeze(), torch.tensor(label[i:i+len(label)//3])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/workspace/Walk_Video_PyTorch/project/misc/positive_rate.ipynb 单元格 14\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f77616c6b5f766964656f5f746f726368312e3132222c2273657474696e6773223a7b22686f7374223a227373683a2f2f4c455f4c6162227d7d/workspace/Walk_Video_PyTorch/project/misc/positive_rate.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m number_list \u001b[39m=\u001b[39m []\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f77616c6b5f766964656f5f746f726368312e3132222c2273657474696e6773223a7b22686f7374223a227373683a2f2f4c455f4c6162227d7d/workspace/Walk_Video_PyTorch/project/misc/positive_rate.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfor\u001b[39;00m i, _ \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(name):\n\u001b[0;32m----> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f77616c6b5f766964656f5f746f726368312e3132222c2273657474696e6773223a7b22686f7374223a227373683a2f2f4c455f4c6162227d7d/workspace/Walk_Video_PyTorch/project/misc/positive_rate.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=7'>8</a>\u001b[0m     number \u001b[39m=\u001b[39m _\u001b[39m.\u001b[39;49msplit(\u001b[39m'\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m'\u001b[39;49m)[\u001b[39m1\u001b[39;49m]\u001b[39m.\u001b[39msplit(\u001b[39m'\u001b[39m\u001b[39m.\u001b[39m\u001b[39m'\u001b[39m)[\u001b[39m0\u001b[39m]\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f77616c6b5f766964656f5f746f726368312e3132222c2273657474696e6773223a7b22686f7374223a227373683a2f2f4c455f4c6162227d7d/workspace/Walk_Video_PyTorch/project/misc/positive_rate.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=8'>9</a>\u001b[0m     number_list\u001b[39m.\u001b[39mappend(number)\n\u001b[1;32m     <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f77616c6b5f766964656f5f746f726368312e3132222c2273657474696e6773223a7b22686f7374223a227373683a2f2f4c455f4c6162227d7d/workspace/Walk_Video_PyTorch/project/misc/positive_rate.ipynb#X15sdnNjb2RlLXJlbW90ZQ%3D%3D?line=10'>11</a>\u001b[0m category_names \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mset\u001b[39m(number_list))\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "number_list = []\n",
    "\n",
    "for i, _ in enumerate(name):\n",
    "    number = _.split('-')[1].split('.')[0]\n",
    "    number_list.append(number)\n",
    "\n",
    "category_names = list(set(number_list))\n",
    "category_names.sort()\n",
    "\n",
    "results = {\n",
    "    '': list(Counter(number_list).values())\n",
    "}\n",
    "\n",
    "\n",
    "def survey(results, category_names, pred: list, label: list, video_num: int):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    results : dict\n",
    "        A mapping from question labels to a list of answers per category.\n",
    "        It is assumed all lists contain the same number of entries and that\n",
    "        it matches the length of *category_names*.\n",
    "    category_names : list of str\n",
    "        The category labels.\n",
    "    \"\"\"\n",
    "    labels = list(results.keys())\n",
    "    data = np.array(list(results.values()))\n",
    "    data_cum = data.cumsum(axis=1)\n",
    "    category_colors = plt.colormaps['RdYlGn'](\n",
    "        np.linspace(0.15, 0.85, data.shape[1]))\n",
    "\n",
    "    fig, ax = plt.subplots(nrows=2, gridspec_kw={'height_ratios': [1.5, 0.5]})\n",
    "    ax[1].invert_yaxis()\n",
    "    ax[1].set_xlim(0, np.sum(data, axis=1).max())\n",
    "\n",
    "    for i, (colname, color) in enumerate(zip(category_names, category_colors)):\n",
    "        print((colname, color))\n",
    "        widths = data[:, i]\n",
    "        starts = data_cum[:, i] - widths\n",
    "\n",
    "        rects = ax[1].barh(labels, widths, left=starts, height=0.1,\n",
    "                           label=colname, color=color)\n",
    "\n",
    "        r, g, b, _ = color\n",
    "        text_color = 'white' if r * g * b < 0.5 else 'darkgrey'\n",
    "\n",
    "        ax[1].bar_label(rects, label_type='center', color=text_color)\n",
    "\n",
    "    ax[1].set_title('The number of video')\n",
    "\n",
    "    # 生成图形\n",
    "    ax[0].plot(video_number, pred, 'bo:', label='predict label', linewidth=2)  # 颜色绿色，点形圆形，线性虚线，设置图例显示内容，线条宽度为2\n",
    "    ax[0].plot(video_number, label, 'r.', label='true label', linewidth=1)\n",
    "\n",
    "    ax[0].legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)  # 显示图例, 图例中内容由 label 定义\n",
    "    # ax[0].set_title('%s positive rate' % name[0].split('-')[0])  # 图形的标题\n",
    "    ax[0].set_ylabel('label')\n",
    "    ax[0].autoscale(enable=True, axis=\"x\", tight=True)\n",
    "\n",
    "    fig.subplots_adjust(hspace=0.5) # 甚至两个图片中间的距离\n",
    "\n",
    "    return fig, ax\n",
    "\n",
    "\n",
    "# 生成数据\n",
    "video_number = range(0, len(label))\n",
    "pred = pred_list\n",
    "label = label\n",
    "\n",
    "fig, ax = survey(results, category_names, pred, label, video_number)\n",
    "\n",
    "# ax[0].ylabel('label') # 横坐标轴的标题\n",
    "# ax[0].xlabel('video number') # 纵坐标轴的标题\n",
    "# ax[0].xticks(range(0, len(label))) # 设置横坐标轴的刻度为 0 到 10 的数组\n",
    "# ax[0].ylim([-0.25, 1.25]) # 设置纵坐标轴范围为 -2 到 2\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # delete the start and finish 1s in raw video\n",
    "# import os \n",
    "\n",
    "# from torchvision.io import read_video, write_video\n",
    "# data_path='/workspace/Walk_Video_PyTorch/project/misc/test_video/'\n",
    "\n",
    "\n",
    "# def slice_start_end_1s(path):\n",
    "#     full_path = os.path.join(data_path, 'ASD', path)\n",
    "#     f, a, info = read_video(full_path, pts_unit='sec')\n",
    "#     write_video(filename=full_path, video_array=f[30:-30], fps=30)\n",
    "#     print('write video:%s' % full_path)\n",
    "\n",
    "\n",
    "# path = os.listdir(data_path+'ASD')\n",
    "# for p in path:\n",
    "#     slice_start_end_1s(p)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grad CAM pytorch \n",
    "use pytorch_grad_cam api to make visilizaiton."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_grad_cam import GradCAM, HiResCAM, FullGrad, GradCAMPlusPlus, AblationCAM, ScoreCAM, LayerCAM\n",
    "from pytorch_grad_cam.utils.model_targets import ClassifierOutputTarget\n",
    "from pytorch_grad_cam.utils.image import show_cam_on_image\n",
    "\n",
    "# guided grad cam method\n",
    "target_layer = [model.model.blocks[-2].res_blocks[-1]]\n",
    "# target_layer = [ model.model.blocks[-2]]\n",
    "\n",
    "cam = LayerCAM(model, target_layer, use_cuda=True)\n",
    "# cam = GradCAM(model, target_layer, use_cuda=True)\n",
    "targets = [ClassifierOutputTarget(-1)]\n",
    "\n",
    "grayscale_cam = cam(video[:20], aug_smooth=True, eigen_smooth=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from captum.attr import visualization as viz\n",
    "import numpy as np \n",
    "\n",
    "def show_cam(video:torch.Tensor, cam):\n",
    "\n",
    "    b, h, w, t = grayscale_cam.shape\n",
    "\n",
    "    for i in range(b):\n",
    "        cam_map = cam[i].mean(axis=2).squeeze()\n",
    "        # cam_map = grayscale_cam[:,:,:,0].squeeze()\n",
    "        cam_map = np.expand_dims(cam_map, 2)\n",
    "\n",
    "        inp_tensor = video[i,:,1,:].permute(1,2,0).cpu().detach().numpy()\n",
    "\n",
    "        # use captum visual method\n",
    "        figure, axis = viz.visualize_image_attr_multiple(\n",
    "            cam_map,\n",
    "            inp_tensor,\n",
    "            methods=['original_image', 'blended_heat_map'],\n",
    "            signs=['all', 'positive'],\n",
    "            show_colorbar=True,\n",
    "            outlier_perc=1, \n",
    "            cmap='jet',\n",
    "            titles=['original image', 'GradCAM++, label %s' % int(1)]\n",
    "        )\n",
    "\n",
    "    return figure, axis\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axis = show_cam(video, grayscale_cam)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f9f85f796d01129d0dd105a088854619f454435301f6ffec2fea96ecbd9be4ac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
