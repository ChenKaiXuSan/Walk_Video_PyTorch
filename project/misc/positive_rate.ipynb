{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Errno 2] No such file or directory: '/ workspace / Walk_Video_PyTorch / project'\n",
      "/workspace/Walk_Video_PyTorch/project/misc\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from pytorchvideo.transforms import (\n",
    "    ApplyTransformToKey,\n",
    "    Normalize,\n",
    "    RandomShortSideScale,\n",
    "    ShortSideScale,\n",
    "    UniformTemporalSubsample,\n",
    "    Div255,\n",
    ")\n",
    "from torchvision.transforms import (\n",
    "    Compose,\n",
    "    Lambda,\n",
    "    RandomCrop,\n",
    "    Resize,\n",
    "    RandomHorizontalFlip,\n",
    ")\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import os\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"1\"\n",
    "\n",
    "\n",
    "%cd / workspace / Walk_Video_PyTorch / project\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Bagua cannot detect bundled NCCL library, Bagua will try to use system NCCL instead. If you encounter any error, please run `import bagua_core; bagua_core.install_deps()` or the `bagua_install_deps.py` script to install bundled libraries.\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'models'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m/workspace/Walk_Video_PyTorch/project/misc/positive_rate.ipynb 单元格 2\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f77616c6b5f766964656f5f746f726368312e3132222c2273657474696e6773223a7b22686f7374223a227373683a2f2f4c455f4c6162227d7d/workspace/Walk_Video_PyTorch/project/misc/positive_rate.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m \u001b[39mimport\u001b[39;00m seed_everything\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f77616c6b5f766964656f5f746f726368312e3132222c2273657474696e6773223a7b22686f7374223a227373683a2f2f4c455f4c6162227d7d/workspace/Walk_Video_PyTorch/project/misc/positive_rate.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mtorchmetrics\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mclassification\u001b[39;00m \u001b[39mimport\u001b[39;00m BinaryAccuracy\n\u001b[0;32m----> <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f77616c6b5f766964656f5f746f726368312e3132222c2273657474696e6773223a7b22686f7374223a227373683a2f2f4c455f4c6162227d7d/workspace/Walk_Video_PyTorch/project/misc/positive_rate.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mmodels\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mpytorchvideo_models\u001b[39;00m \u001b[39mimport\u001b[39;00m WalkVideoClassificationLightningModule\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f77616c6b5f766964656f5f746f726368312e3132222c2273657474696e6773223a7b22686f7374223a227373683a2f2f4c455f4c6162227d7d/workspace/Walk_Video_PyTorch/project/misc/positive_rate.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mdataloader\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata_loader\u001b[39;00m \u001b[39mimport\u001b[39;00m WalkDataModule\n\u001b[1;32m      <a href='vscode-notebook-cell://attached-container%2B7b22636f6e7461696e65724e616d65223a222f77616c6b5f766964656f5f746f726368312e3132222c2273657474696e6773223a7b22686f7374223a227373683a2f2f4c455f4c6162227d7d/workspace/Walk_Video_PyTorch/project/misc/positive_rate.ipynb#W1sdnNjb2RlLXJlbW90ZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpytorch_lightning\u001b[39;00m \u001b[39mimport\u001b[39;00m Trainer\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'models'"
     ]
    }
   ],
   "source": [
    "from pytorch_lightning import seed_everything\n",
    "from torchmetrics.classification import BinaryAccuracy\n",
    "from models.pytorchvideo_models import WalkVideoClassificationLightningModule\n",
    "from dataloader.data_loader import WalkDataModule\n",
    "\n",
    "from pytorch_lightning import Trainer\n",
    "\n",
    "from IPython.display import clear_output\n",
    "\n",
    "clear_output()\n",
    "\n",
    "\n",
    "seed_everything(42, workers=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from parameters import get_parameters\n",
    "\n",
    "VIDEO_LENGTH = ['1', '2', '3']\n",
    "VIDEO_FRAME = ['8', '16', '32']\n",
    "\n",
    "opt, _ = get_parameters()\n",
    "opt.num_workers = 8\n",
    "opt.batch_size = 8\n",
    "opt.gpu_num = 1\n",
    "\n",
    "opt.version = '1201_1_16'\n",
    "opt.model = \"resnet\"\n",
    "opt.model_depth = 50\n",
    "opt.model_class_num = 1\n",
    "\n",
    "# opt.clip_duration = 2\n",
    "# opt.uniform_temporal_subsample_num = 30\n",
    "opt.version = opt.version + '_' + opt.model + '_depth' + str(opt.model_depth)\n",
    "\n",
    "opt.fusion_method = 'slow_fusion'\n",
    "opt.fix_layer = 'all'\n",
    "# opt.train_path = '/workspace/data/split_pad_dataset_512/fold3/'\n",
    "\n",
    "opt.transfor_learning = True\n",
    "opt.pre_process_flag = True\n",
    "\n",
    "DATA_PATH = opt.split_pad_data_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt.train_path = os.path.join(DATA_PATH, 'flod2')\n",
    "\n",
    "opt.clip_duration = int(1)\n",
    "opt.uniform_temporal_subsample_num = int(8)\n",
    "\n",
    "# ckpt_path = '/workspace/Walk_Video_PyTorch/logs/resnet/1119_1_8_resnet_depth50/flod2/checkpoints/epoch=11-val_loss=0.63-val_acc=0.8926.ckpt'\n",
    "ckpt_path = '/workspace/Walk_Video_PyTorch/logs/resnet/1119_1_8_resnet_depth50/flod3/checkpoints/epoch=23-val_loss=0.47-val_acc=0.8568.ckpt'\n",
    "\n",
    "print('#' * 50)\n",
    "print('the data path: %s' % opt.train_path)\n",
    "print('ckpt: %s' % ckpt_path)\n",
    "model = WalkVideoClassificationLightningModule(opt).load_from_checkpoint(ckpt_path)\n",
    "\n",
    "data_module = WalkDataModule(opt)\n",
    "data_module.setup()\n",
    "test_data = data_module.val_dataloader()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = Compose(\n",
    "    [\n",
    "        ApplyTransformToKey(\n",
    "            key=\"video\",\n",
    "            transform=Compose(\n",
    "                [\n",
    "                    # uniform clip T frames from the given n sec video.\n",
    "                    UniformTemporalSubsample(8),\n",
    "\n",
    "                    # dived the pixel from [0, 255] tp [0, 1], to save computing resources.\n",
    "                    Div255(),\n",
    "                    Normalize((0.45, 0.45, 0.45), (0.225, 0.225, 0.225)),\n",
    "\n",
    "                    Resize(size=[224, 224]),\n",
    "                    RandomHorizontalFlip(p=0.5),\n",
    "                ]\n",
    "            ),\n",
    "        ),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from data_loader import WalkDataset\n",
    "from pytorchvideo.data import make_clip_sampler\n",
    "\n",
    "dset = WalkDataset(\n",
    "    data_path='/workspace/Walk_Video_PyTorch/project/misc/test_video/',\n",
    "    clip_sampler=make_clip_sampler('uniform', 1),\n",
    "    video_sampler=torch.utils.data.SequentialSampler,\n",
    "    transform=transform,\n",
    ")\n",
    "\n",
    "test_data = DataLoader(\n",
    "    dset,\n",
    "    batch_size=300\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(test_data))\n",
    "\n",
    "video = batch['video'].detach()  # b, c, t, h, w\n",
    "label = batch['label'].detach()  # b, class num\n",
    "label = [1] * len(label)\n",
    "name = batch['video_name']\n",
    "\n",
    "# video.shape, label, name, len(name)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred the result\n",
    "model = model.cuda()\n",
    "video = video.cuda()\n",
    "\n",
    "with torch.no_grad():\n",
    "    pred = model(video)\n",
    "\n",
    "pred = pred.cpu()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred_list = []\n",
    "\n",
    "for i in torch.sigmoid(pred):\n",
    "    if i > 0.5:\n",
    "        pred_list.append(1)\n",
    "    else:\n",
    "        pred_list.append(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pred_list, label,\n",
    "len(label)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calc acc\n",
    "acc = BinaryAccuracy(threshold=0.5)\n",
    "\n",
    "acc(torch.sigmoid(pred).squeeze(), torch.tensor(label))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "number_list = []\n",
    "\n",
    "for i, _ in enumerate(name):\n",
    "    number = _.split('-')[1].split('.')[0]\n",
    "    number_list.append(number)\n",
    "\n",
    "category_names = list(set(number_list))\n",
    "category_names.sort()\n",
    "\n",
    "results = {\n",
    "    '': list(Counter(number_list).values())\n",
    "}\n",
    "\n",
    "\n",
    "def survey(results, category_names, pred: list, label: list, video_num: int):\n",
    "    \"\"\"\n",
    "    Parameters\n",
    "    ----------\n",
    "    results : dict\n",
    "        A mapping from question labels to a list of answers per category.\n",
    "        It is assumed all lists contain the same number of entries and that\n",
    "        it matches the length of *category_names*.\n",
    "    category_names : list of str\n",
    "        The category labels.\n",
    "    \"\"\"\n",
    "    labels = list(results.keys())\n",
    "    data = np.array(list(results.values()))\n",
    "    data_cum = data.cumsum(axis=1)\n",
    "    category_colors = plt.colormaps['RdYlGn'](\n",
    "        np.linspace(0.15, 0.85, data.shape[1]))\n",
    "\n",
    "    fig, ax = plt.subplots(nrows=2,\n",
    "                           height_ratios=[1.5, 0.5],\n",
    "                           gridspec_kw={'height_ratios': [1.5, 0.5]})\n",
    "    ax[1].invert_yaxis()\n",
    "    ax[1].set_xlim(0, np.sum(data, axis=1).max())\n",
    "\n",
    "    for i, (colname, color) in enumerate(zip(category_names, category_colors)):\n",
    "        print((colname, color))\n",
    "        widths = data[:, i]\n",
    "        starts = data_cum[:, i] - widths\n",
    "\n",
    "        rects = ax[1].barh(labels, widths, left=starts, height=0.1,\n",
    "                           label=colname, color=color)\n",
    "\n",
    "        r, g, b, _ = color\n",
    "        text_color = 'white' if r * g * b < 0.5 else 'darkgrey'\n",
    "\n",
    "        ax[1].bar_label(rects, label_type='center', color=text_color)\n",
    "\n",
    "    ax[1].set_title('video number')\n",
    "\n",
    "    # 生成图形\n",
    "    ax[0].plot(video_number, pred, 'bo:', label='predict label', linewidth=2)  # 颜色绿色，点形圆形，线性虚线，设置图例显示内容，线条宽度为2\n",
    "    ax[0].plot(video_number, label, 'r.', label='true label', linewidth=1)\n",
    "\n",
    "    # ax[0].ylabel('label') # 横坐标轴的标题\n",
    "    # ax[0].xlabel('video number') # 纵坐标轴的标题\n",
    "    # ax[0].xticks(range(0, len(label))) # 设置横坐标轴的刻度为 0 到 10 的数组\n",
    "    # ax[0].ylim([-0.25, 1.25]) # 设置纵坐标轴范围为 -2 到 2\n",
    "    ax[0].legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)  # 显示图例, 图例中内容由 label 定义\n",
    "    ax[0].set_title('%s positive rate' % name[0].split('-')[0])  # 图形的标题\n",
    "    ax[0].set_ylabel('label')\n",
    "    ax[0].autoscale(enable=True, axis=\"x\", tight=True)\n",
    "    fig.subplots_adjust(hspace=0.5)\n",
    "\n",
    "    return fig, ax\n",
    "\n",
    "\n",
    "# 生成数据\n",
    "video_number = range(0, len(label))\n",
    "pred = pred_list\n",
    "label = label\n",
    "\n",
    "fig, ax = survey(results, category_names)\n",
    "\n",
    "# 生成图形\n",
    "ax[0].plot(video_number, pred, 'bo:', label='predict label', linewidth=2)  # 颜色绿色，点形圆形，线性虚线，设置图例显示内容，线条宽度为2\n",
    "ax[0].plot(video_number, label, 'r.', label='true label', linewidth=1)\n",
    "\n",
    "# ax[0].ylabel('label') # 横坐标轴的标题\n",
    "# ax[0].xlabel('video number') # 纵坐标轴的标题\n",
    "# ax[0].xticks(range(0, len(label))) # 设置横坐标轴的刻度为 0 到 10 的数组\n",
    "# ax[0].ylim([-0.25, 1.25]) # 设置纵坐标轴范围为 -2 到 2\n",
    "ax[0].legend(bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)  # 显示图例, 图例中内容由 label 定义\n",
    "ax[0].set_title('%s positive rate' % name[0].split('-')[0])  # 图形的标题\n",
    "ax[0].set_ylabel('label')\n",
    "ax[0].autoscale(enable=True, axis=\"x\", tight=True)\n",
    "fig.subplots_adjust(hspace=0.5)\n",
    "\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f9f85f796d01129d0dd105a088854619f454435301f6ffec2fea96ecbd9be4ac"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
